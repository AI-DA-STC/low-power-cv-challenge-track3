{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPTForDepthEstimation\n",
    "from safetensors.torch import load_file\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/coder/low-power-cv-challenge-track3/outputs/checkpoint-7000/ were not used when initializing DPTForDepthEstimation: ['module.model.backbone.embeddings.cls_token', 'module.model.backbone.embeddings.mask_token', 'module.model.backbone.embeddings.patch_embeddings.projection.bias', 'module.model.backbone.embeddings.patch_embeddings.projection.weight', 'module.model.backbone.embeddings.position_embeddings', 'module.model.backbone.encoder.layer.0.attention.attention.key.bias', 'module.model.backbone.encoder.layer.0.attention.attention.key.weight', 'module.model.backbone.encoder.layer.0.attention.attention.query.bias', 'module.model.backbone.encoder.layer.0.attention.attention.query.weight', 'module.model.backbone.encoder.layer.0.attention.attention.value.bias', 'module.model.backbone.encoder.layer.0.attention.attention.value.weight', 'module.model.backbone.encoder.layer.0.attention.output.dense.bias', 'module.model.backbone.encoder.layer.0.attention.output.dense.weight', 'module.model.backbone.encoder.layer.0.layer_scale1.lambda1', 'module.model.backbone.encoder.layer.0.layer_scale2.lambda1', 'module.model.backbone.encoder.layer.0.mlp.fc1.bias', 'module.model.backbone.encoder.layer.0.mlp.fc1.weight', 'module.model.backbone.encoder.layer.0.mlp.fc2.bias', 'module.model.backbone.encoder.layer.0.mlp.fc2.weight', 'module.model.backbone.encoder.layer.0.norm1.bias', 'module.model.backbone.encoder.layer.0.norm1.weight', 'module.model.backbone.encoder.layer.0.norm2.bias', 'module.model.backbone.encoder.layer.0.norm2.weight', 'module.model.backbone.encoder.layer.1.attention.attention.key.bias', 'module.model.backbone.encoder.layer.1.attention.attention.key.weight', 'module.model.backbone.encoder.layer.1.attention.attention.query.bias', 'module.model.backbone.encoder.layer.1.attention.attention.query.weight', 'module.model.backbone.encoder.layer.1.attention.attention.value.bias', 'module.model.backbone.encoder.layer.1.attention.attention.value.weight', 'module.model.backbone.encoder.layer.1.attention.output.dense.bias', 'module.model.backbone.encoder.layer.1.attention.output.dense.weight', 'module.model.backbone.encoder.layer.1.layer_scale1.lambda1', 'module.model.backbone.encoder.layer.1.layer_scale2.lambda1', 'module.model.backbone.encoder.layer.1.mlp.fc1.bias', 'module.model.backbone.encoder.layer.1.mlp.fc1.weight', 'module.model.backbone.encoder.layer.1.mlp.fc2.bias', 'module.model.backbone.encoder.layer.1.mlp.fc2.weight', 'module.model.backbone.encoder.layer.1.norm1.bias', 'module.model.backbone.encoder.layer.1.norm1.weight', 'module.model.backbone.encoder.layer.1.norm2.bias', 'module.model.backbone.encoder.layer.1.norm2.weight', 'module.model.backbone.encoder.layer.10.attention.attention.key.bias', 'module.model.backbone.encoder.layer.10.attention.attention.key.weight', 'module.model.backbone.encoder.layer.10.attention.attention.query.bias', 'module.model.backbone.encoder.layer.10.attention.attention.query.weight', 'module.model.backbone.encoder.layer.10.attention.attention.value.bias', 'module.model.backbone.encoder.layer.10.attention.attention.value.weight', 'module.model.backbone.encoder.layer.10.attention.output.dense.bias', 'module.model.backbone.encoder.layer.10.attention.output.dense.weight', 'module.model.backbone.encoder.layer.10.layer_scale1.lambda1', 'module.model.backbone.encoder.layer.10.layer_scale2.lambda1', 'module.model.backbone.encoder.layer.10.mlp.fc1.bias', 'module.model.backbone.encoder.layer.10.mlp.fc1.weight', 'module.model.backbone.encoder.layer.10.mlp.fc2.bias', 'module.model.backbone.encoder.layer.10.mlp.fc2.weight', 'module.model.backbone.encoder.layer.10.norm1.bias', 'module.model.backbone.encoder.layer.10.norm1.weight', 'module.model.backbone.encoder.layer.10.norm2.bias', 'module.model.backbone.encoder.layer.10.norm2.weight', 'module.model.backbone.encoder.layer.11.attention.attention.key.bias', 'module.model.backbone.encoder.layer.11.attention.attention.key.weight', 'module.model.backbone.encoder.layer.11.attention.attention.query.bias', 'module.model.backbone.encoder.layer.11.attention.attention.query.weight', 'module.model.backbone.encoder.layer.11.attention.attention.value.bias', 'module.model.backbone.encoder.layer.11.attention.attention.value.weight', 'module.model.backbone.encoder.layer.11.attention.output.dense.bias', 'module.model.backbone.encoder.layer.11.attention.output.dense.weight', 'module.model.backbone.encoder.layer.11.layer_scale1.lambda1', 'module.model.backbone.encoder.layer.11.layer_scale2.lambda1', 'module.model.backbone.encoder.layer.11.mlp.fc1.bias', 'module.model.backbone.encoder.layer.11.mlp.fc1.weight', 'module.model.backbone.encoder.layer.11.mlp.fc2.bias', 'module.model.backbone.encoder.layer.11.mlp.fc2.weight', 'module.model.backbone.encoder.layer.11.norm1.bias', 'module.model.backbone.encoder.layer.11.norm1.weight', 'module.model.backbone.encoder.layer.11.norm2.bias', 'module.model.backbone.encoder.layer.11.norm2.weight', 'module.model.backbone.encoder.layer.2.attention.attention.key.bias', 'module.model.backbone.encoder.layer.2.attention.attention.key.weight', 'module.model.backbone.encoder.layer.2.attention.attention.query.bias', 'module.model.backbone.encoder.layer.2.attention.attention.query.weight', 'module.model.backbone.encoder.layer.2.attention.attention.value.bias', 'module.model.backbone.encoder.layer.2.attention.attention.value.weight', 'module.model.backbone.encoder.layer.2.attention.output.dense.bias', 'module.model.backbone.encoder.layer.2.attention.output.dense.weight', 'module.model.backbone.encoder.layer.2.layer_scale1.lambda1', 'module.model.backbone.encoder.layer.2.layer_scale2.lambda1', 'module.model.backbone.encoder.layer.2.mlp.fc1.bias', 'module.model.backbone.encoder.layer.2.mlp.fc1.weight', 'module.model.backbone.encoder.layer.2.mlp.fc2.bias', 'module.model.backbone.encoder.layer.2.mlp.fc2.weight', 'module.model.backbone.encoder.layer.2.norm1.bias', 'module.model.backbone.encoder.layer.2.norm1.weight', 'module.model.backbone.encoder.layer.2.norm2.bias', 'module.model.backbone.encoder.layer.2.norm2.weight', 'module.model.backbone.encoder.layer.3.attention.attention.key.bias', 'module.model.backbone.encoder.layer.3.attention.attention.key.weight', 'module.model.backbone.encoder.layer.3.attention.attention.query.bias', 'module.model.backbone.encoder.layer.3.attention.attention.query.weight', 'module.model.backbone.encoder.layer.3.attention.attention.value.bias', 'module.model.backbone.encoder.layer.3.attention.attention.value.weight', 'module.model.backbone.encoder.layer.3.attention.output.dense.bias', 'module.model.backbone.encoder.layer.3.attention.output.dense.weight', 'module.model.backbone.encoder.layer.3.layer_scale1.lambda1', 'module.model.backbone.encoder.layer.3.layer_scale2.lambda1', 'module.model.backbone.encoder.layer.3.mlp.fc1.bias', 'module.model.backbone.encoder.layer.3.mlp.fc1.weight', 'module.model.backbone.encoder.layer.3.mlp.fc2.bias', 'module.model.backbone.encoder.layer.3.mlp.fc2.weight', 'module.model.backbone.encoder.layer.3.norm1.bias', 'module.model.backbone.encoder.layer.3.norm1.weight', 'module.model.backbone.encoder.layer.3.norm2.bias', 'module.model.backbone.encoder.layer.3.norm2.weight', 'module.model.backbone.encoder.layer.4.attention.attention.key.bias', 'module.model.backbone.encoder.layer.4.attention.attention.key.weight', 'module.model.backbone.encoder.layer.4.attention.attention.query.bias', 'module.model.backbone.encoder.layer.4.attention.attention.query.weight', 'module.model.backbone.encoder.layer.4.attention.attention.value.bias', 'module.model.backbone.encoder.layer.4.attention.attention.value.weight', 'module.model.backbone.encoder.layer.4.attention.output.dense.bias', 'module.model.backbone.encoder.layer.4.attention.output.dense.weight', 'module.model.backbone.encoder.layer.4.layer_scale1.lambda1', 'module.model.backbone.encoder.layer.4.layer_scale2.lambda1', 'module.model.backbone.encoder.layer.4.mlp.fc1.bias', 'module.model.backbone.encoder.layer.4.mlp.fc1.weight', 'module.model.backbone.encoder.layer.4.mlp.fc2.bias', 'module.model.backbone.encoder.layer.4.mlp.fc2.weight', 'module.model.backbone.encoder.layer.4.norm1.bias', 'module.model.backbone.encoder.layer.4.norm1.weight', 'module.model.backbone.encoder.layer.4.norm2.bias', 'module.model.backbone.encoder.layer.4.norm2.weight', 'module.model.backbone.encoder.layer.5.attention.attention.key.bias', 'module.model.backbone.encoder.layer.5.attention.attention.key.weight', 'module.model.backbone.encoder.layer.5.attention.attention.query.bias', 'module.model.backbone.encoder.layer.5.attention.attention.query.weight', 'module.model.backbone.encoder.layer.5.attention.attention.value.bias', 'module.model.backbone.encoder.layer.5.attention.attention.value.weight', 'module.model.backbone.encoder.layer.5.attention.output.dense.bias', 'module.model.backbone.encoder.layer.5.attention.output.dense.weight', 'module.model.backbone.encoder.layer.5.layer_scale1.lambda1', 'module.model.backbone.encoder.layer.5.layer_scale2.lambda1', 'module.model.backbone.encoder.layer.5.mlp.fc1.bias', 'module.model.backbone.encoder.layer.5.mlp.fc1.weight', 'module.model.backbone.encoder.layer.5.mlp.fc2.bias', 'module.model.backbone.encoder.layer.5.mlp.fc2.weight', 'module.model.backbone.encoder.layer.5.norm1.bias', 'module.model.backbone.encoder.layer.5.norm1.weight', 'module.model.backbone.encoder.layer.5.norm2.bias', 'module.model.backbone.encoder.layer.5.norm2.weight', 'module.model.backbone.encoder.layer.6.attention.attention.key.bias', 'module.model.backbone.encoder.layer.6.attention.attention.key.weight', 'module.model.backbone.encoder.layer.6.attention.attention.query.bias', 'module.model.backbone.encoder.layer.6.attention.attention.query.weight', 'module.model.backbone.encoder.layer.6.attention.attention.value.bias', 'module.model.backbone.encoder.layer.6.attention.attention.value.weight', 'module.model.backbone.encoder.layer.6.attention.output.dense.bias', 'module.model.backbone.encoder.layer.6.attention.output.dense.weight', 'module.model.backbone.encoder.layer.6.layer_scale1.lambda1', 'module.model.backbone.encoder.layer.6.layer_scale2.lambda1', 'module.model.backbone.encoder.layer.6.mlp.fc1.bias', 'module.model.backbone.encoder.layer.6.mlp.fc1.weight', 'module.model.backbone.encoder.layer.6.mlp.fc2.bias', 'module.model.backbone.encoder.layer.6.mlp.fc2.weight', 'module.model.backbone.encoder.layer.6.norm1.bias', 'module.model.backbone.encoder.layer.6.norm1.weight', 'module.model.backbone.encoder.layer.6.norm2.bias', 'module.model.backbone.encoder.layer.6.norm2.weight', 'module.model.backbone.encoder.layer.7.attention.attention.key.bias', 'module.model.backbone.encoder.layer.7.attention.attention.key.weight', 'module.model.backbone.encoder.layer.7.attention.attention.query.bias', 'module.model.backbone.encoder.layer.7.attention.attention.query.weight', 'module.model.backbone.encoder.layer.7.attention.attention.value.bias', 'module.model.backbone.encoder.layer.7.attention.attention.value.weight', 'module.model.backbone.encoder.layer.7.attention.output.dense.bias', 'module.model.backbone.encoder.layer.7.attention.output.dense.weight', 'module.model.backbone.encoder.layer.7.layer_scale1.lambda1', 'module.model.backbone.encoder.layer.7.layer_scale2.lambda1', 'module.model.backbone.encoder.layer.7.mlp.fc1.bias', 'module.model.backbone.encoder.layer.7.mlp.fc1.weight', 'module.model.backbone.encoder.layer.7.mlp.fc2.bias', 'module.model.backbone.encoder.layer.7.mlp.fc2.weight', 'module.model.backbone.encoder.layer.7.norm1.bias', 'module.model.backbone.encoder.layer.7.norm1.weight', 'module.model.backbone.encoder.layer.7.norm2.bias', 'module.model.backbone.encoder.layer.7.norm2.weight', 'module.model.backbone.encoder.layer.8.attention.attention.key.bias', 'module.model.backbone.encoder.layer.8.attention.attention.key.weight', 'module.model.backbone.encoder.layer.8.attention.attention.query.bias', 'module.model.backbone.encoder.layer.8.attention.attention.query.weight', 'module.model.backbone.encoder.layer.8.attention.attention.value.bias', 'module.model.backbone.encoder.layer.8.attention.attention.value.weight', 'module.model.backbone.encoder.layer.8.attention.output.dense.bias', 'module.model.backbone.encoder.layer.8.attention.output.dense.weight', 'module.model.backbone.encoder.layer.8.layer_scale1.lambda1', 'module.model.backbone.encoder.layer.8.layer_scale2.lambda1', 'module.model.backbone.encoder.layer.8.mlp.fc1.bias', 'module.model.backbone.encoder.layer.8.mlp.fc1.weight', 'module.model.backbone.encoder.layer.8.mlp.fc2.bias', 'module.model.backbone.encoder.layer.8.mlp.fc2.weight', 'module.model.backbone.encoder.layer.8.norm1.bias', 'module.model.backbone.encoder.layer.8.norm1.weight', 'module.model.backbone.encoder.layer.8.norm2.bias', 'module.model.backbone.encoder.layer.8.norm2.weight', 'module.model.backbone.encoder.layer.9.attention.attention.key.bias', 'module.model.backbone.encoder.layer.9.attention.attention.key.weight', 'module.model.backbone.encoder.layer.9.attention.attention.query.bias', 'module.model.backbone.encoder.layer.9.attention.attention.query.weight', 'module.model.backbone.encoder.layer.9.attention.attention.value.bias', 'module.model.backbone.encoder.layer.9.attention.attention.value.weight', 'module.model.backbone.encoder.layer.9.attention.output.dense.bias', 'module.model.backbone.encoder.layer.9.attention.output.dense.weight', 'module.model.backbone.encoder.layer.9.layer_scale1.lambda1', 'module.model.backbone.encoder.layer.9.layer_scale2.lambda1', 'module.model.backbone.encoder.layer.9.mlp.fc1.bias', 'module.model.backbone.encoder.layer.9.mlp.fc1.weight', 'module.model.backbone.encoder.layer.9.mlp.fc2.bias', 'module.model.backbone.encoder.layer.9.mlp.fc2.weight', 'module.model.backbone.encoder.layer.9.norm1.bias', 'module.model.backbone.encoder.layer.9.norm1.weight', 'module.model.backbone.encoder.layer.9.norm2.bias', 'module.model.backbone.encoder.layer.9.norm2.weight', 'module.model.backbone.layernorm.bias', 'module.model.backbone.layernorm.weight', 'module.model.head.head.0.bias', 'module.model.head.head.0.weight', 'module.model.head.head.2.bias', 'module.model.head.head.2.weight', 'module.model.head.head.4.bias', 'module.model.head.head.4.weight', 'module.model.head.projection.bias', 'module.model.head.projection.weight', 'module.model.neck.convs.0.weight', 'module.model.neck.convs.1.weight', 'module.model.neck.convs.2.weight', 'module.model.neck.convs.3.weight', 'module.model.neck.fusion_stage.layers.0.projection.bias', 'module.model.neck.fusion_stage.layers.0.projection.weight', 'module.model.neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'module.model.neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'module.model.neck.fusion_stage.layers.0.residual_layer2.convolution1.weight', 'module.model.neck.fusion_stage.layers.0.residual_layer2.convolution2.weight', 'module.model.neck.fusion_stage.layers.1.projection.bias', 'module.model.neck.fusion_stage.layers.1.projection.weight', 'module.model.neck.fusion_stage.layers.1.residual_layer1.convolution1.weight', 'module.model.neck.fusion_stage.layers.1.residual_layer1.convolution2.weight', 'module.model.neck.fusion_stage.layers.1.residual_layer2.convolution1.weight', 'module.model.neck.fusion_stage.layers.1.residual_layer2.convolution2.weight', 'module.model.neck.fusion_stage.layers.2.projection.bias', 'module.model.neck.fusion_stage.layers.2.projection.weight', 'module.model.neck.fusion_stage.layers.2.residual_layer1.convolution1.weight', 'module.model.neck.fusion_stage.layers.2.residual_layer1.convolution2.weight', 'module.model.neck.fusion_stage.layers.2.residual_layer2.convolution1.weight', 'module.model.neck.fusion_stage.layers.2.residual_layer2.convolution2.weight', 'module.model.neck.fusion_stage.layers.3.projection.bias', 'module.model.neck.fusion_stage.layers.3.projection.weight', 'module.model.neck.fusion_stage.layers.3.residual_layer1.convolution1.weight', 'module.model.neck.fusion_stage.layers.3.residual_layer1.convolution2.weight', 'module.model.neck.fusion_stage.layers.3.residual_layer2.convolution1.weight', 'module.model.neck.fusion_stage.layers.3.residual_layer2.convolution2.weight', 'module.model.neck.reassemble_stage.layers.0.projection.bias', 'module.model.neck.reassemble_stage.layers.0.projection.weight', 'module.model.neck.reassemble_stage.layers.0.resize.bias', 'module.model.neck.reassemble_stage.layers.0.resize.weight', 'module.model.neck.reassemble_stage.layers.1.projection.bias', 'module.model.neck.reassemble_stage.layers.1.projection.weight', 'module.model.neck.reassemble_stage.layers.1.resize.bias', 'module.model.neck.reassemble_stage.layers.1.resize.weight', 'module.model.neck.reassemble_stage.layers.2.projection.bias', 'module.model.neck.reassemble_stage.layers.2.projection.weight', 'module.model.neck.reassemble_stage.layers.3.projection.bias', 'module.model.neck.reassemble_stage.layers.3.projection.weight', 'module.model.neck.reassemble_stage.layers.3.resize.bias', 'module.model.neck.reassemble_stage.layers.3.resize.weight', 'module.model.neck.reassemble_stage.readout_projects.0.0.bias', 'module.model.neck.reassemble_stage.readout_projects.0.0.weight', 'module.model.neck.reassemble_stage.readout_projects.1.0.bias', 'module.model.neck.reassemble_stage.readout_projects.1.0.weight', 'module.model.neck.reassemble_stage.readout_projects.2.0.bias', 'module.model.neck.reassemble_stage.readout_projects.2.0.weight', 'module.model.neck.reassemble_stage.readout_projects.3.0.bias', 'module.model.neck.reassemble_stage.readout_projects.3.0.weight']\n",
      "- This IS expected if you are initializing DPTForDepthEstimation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPTForDepthEstimation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at /home/coder/low-power-cv-challenge-track3/outputs/checkpoint-7000/ and are newly initialized: ['backbone.embeddings.cls_token', 'backbone.embeddings.mask_token', 'backbone.embeddings.patch_embeddings.projection.bias', 'backbone.embeddings.patch_embeddings.projection.weight', 'backbone.embeddings.position_embeddings', 'backbone.encoder.layer.0.attention.attention.key.bias', 'backbone.encoder.layer.0.attention.attention.key.weight', 'backbone.encoder.layer.0.attention.attention.query.bias', 'backbone.encoder.layer.0.attention.attention.query.weight', 'backbone.encoder.layer.0.attention.attention.value.bias', 'backbone.encoder.layer.0.attention.attention.value.weight', 'backbone.encoder.layer.0.attention.output.dense.bias', 'backbone.encoder.layer.0.attention.output.dense.weight', 'backbone.encoder.layer.0.layer_scale1.lambda1', 'backbone.encoder.layer.0.layer_scale2.lambda1', 'backbone.encoder.layer.0.mlp.fc1.bias', 'backbone.encoder.layer.0.mlp.fc1.weight', 'backbone.encoder.layer.0.mlp.fc2.bias', 'backbone.encoder.layer.0.mlp.fc2.weight', 'backbone.encoder.layer.0.norm1.bias', 'backbone.encoder.layer.0.norm1.weight', 'backbone.encoder.layer.0.norm2.bias', 'backbone.encoder.layer.0.norm2.weight', 'backbone.encoder.layer.1.attention.attention.key.bias', 'backbone.encoder.layer.1.attention.attention.key.weight', 'backbone.encoder.layer.1.attention.attention.query.bias', 'backbone.encoder.layer.1.attention.attention.query.weight', 'backbone.encoder.layer.1.attention.attention.value.bias', 'backbone.encoder.layer.1.attention.attention.value.weight', 'backbone.encoder.layer.1.attention.output.dense.bias', 'backbone.encoder.layer.1.attention.output.dense.weight', 'backbone.encoder.layer.1.layer_scale1.lambda1', 'backbone.encoder.layer.1.layer_scale2.lambda1', 'backbone.encoder.layer.1.mlp.fc1.bias', 'backbone.encoder.layer.1.mlp.fc1.weight', 'backbone.encoder.layer.1.mlp.fc2.bias', 'backbone.encoder.layer.1.mlp.fc2.weight', 'backbone.encoder.layer.1.norm1.bias', 'backbone.encoder.layer.1.norm1.weight', 'backbone.encoder.layer.1.norm2.bias', 'backbone.encoder.layer.1.norm2.weight', 'backbone.encoder.layer.10.attention.attention.key.bias', 'backbone.encoder.layer.10.attention.attention.key.weight', 'backbone.encoder.layer.10.attention.attention.query.bias', 'backbone.encoder.layer.10.attention.attention.query.weight', 'backbone.encoder.layer.10.attention.attention.value.bias', 'backbone.encoder.layer.10.attention.attention.value.weight', 'backbone.encoder.layer.10.attention.output.dense.bias', 'backbone.encoder.layer.10.attention.output.dense.weight', 'backbone.encoder.layer.10.layer_scale1.lambda1', 'backbone.encoder.layer.10.layer_scale2.lambda1', 'backbone.encoder.layer.10.mlp.fc1.bias', 'backbone.encoder.layer.10.mlp.fc1.weight', 'backbone.encoder.layer.10.mlp.fc2.bias', 'backbone.encoder.layer.10.mlp.fc2.weight', 'backbone.encoder.layer.10.norm1.bias', 'backbone.encoder.layer.10.norm1.weight', 'backbone.encoder.layer.10.norm2.bias', 'backbone.encoder.layer.10.norm2.weight', 'backbone.encoder.layer.11.attention.attention.key.bias', 'backbone.encoder.layer.11.attention.attention.key.weight', 'backbone.encoder.layer.11.attention.attention.query.bias', 'backbone.encoder.layer.11.attention.attention.query.weight', 'backbone.encoder.layer.11.attention.attention.value.bias', 'backbone.encoder.layer.11.attention.attention.value.weight', 'backbone.encoder.layer.11.attention.output.dense.bias', 'backbone.encoder.layer.11.attention.output.dense.weight', 'backbone.encoder.layer.11.layer_scale1.lambda1', 'backbone.encoder.layer.11.layer_scale2.lambda1', 'backbone.encoder.layer.11.mlp.fc1.bias', 'backbone.encoder.layer.11.mlp.fc1.weight', 'backbone.encoder.layer.11.mlp.fc2.bias', 'backbone.encoder.layer.11.mlp.fc2.weight', 'backbone.encoder.layer.11.norm1.bias', 'backbone.encoder.layer.11.norm1.weight', 'backbone.encoder.layer.11.norm2.bias', 'backbone.encoder.layer.11.norm2.weight', 'backbone.encoder.layer.2.attention.attention.key.bias', 'backbone.encoder.layer.2.attention.attention.key.weight', 'backbone.encoder.layer.2.attention.attention.query.bias', 'backbone.encoder.layer.2.attention.attention.query.weight', 'backbone.encoder.layer.2.attention.attention.value.bias', 'backbone.encoder.layer.2.attention.attention.value.weight', 'backbone.encoder.layer.2.attention.output.dense.bias', 'backbone.encoder.layer.2.attention.output.dense.weight', 'backbone.encoder.layer.2.layer_scale1.lambda1', 'backbone.encoder.layer.2.layer_scale2.lambda1', 'backbone.encoder.layer.2.mlp.fc1.bias', 'backbone.encoder.layer.2.mlp.fc1.weight', 'backbone.encoder.layer.2.mlp.fc2.bias', 'backbone.encoder.layer.2.mlp.fc2.weight', 'backbone.encoder.layer.2.norm1.bias', 'backbone.encoder.layer.2.norm1.weight', 'backbone.encoder.layer.2.norm2.bias', 'backbone.encoder.layer.2.norm2.weight', 'backbone.encoder.layer.3.attention.attention.key.bias', 'backbone.encoder.layer.3.attention.attention.key.weight', 'backbone.encoder.layer.3.attention.attention.query.bias', 'backbone.encoder.layer.3.attention.attention.query.weight', 'backbone.encoder.layer.3.attention.attention.value.bias', 'backbone.encoder.layer.3.attention.attention.value.weight', 'backbone.encoder.layer.3.attention.output.dense.bias', 'backbone.encoder.layer.3.attention.output.dense.weight', 'backbone.encoder.layer.3.layer_scale1.lambda1', 'backbone.encoder.layer.3.layer_scale2.lambda1', 'backbone.encoder.layer.3.mlp.fc1.bias', 'backbone.encoder.layer.3.mlp.fc1.weight', 'backbone.encoder.layer.3.mlp.fc2.bias', 'backbone.encoder.layer.3.mlp.fc2.weight', 'backbone.encoder.layer.3.norm1.bias', 'backbone.encoder.layer.3.norm1.weight', 'backbone.encoder.layer.3.norm2.bias', 'backbone.encoder.layer.3.norm2.weight', 'backbone.encoder.layer.4.attention.attention.key.bias', 'backbone.encoder.layer.4.attention.attention.key.weight', 'backbone.encoder.layer.4.attention.attention.query.bias', 'backbone.encoder.layer.4.attention.attention.query.weight', 'backbone.encoder.layer.4.attention.attention.value.bias', 'backbone.encoder.layer.4.attention.attention.value.weight', 'backbone.encoder.layer.4.attention.output.dense.bias', 'backbone.encoder.layer.4.attention.output.dense.weight', 'backbone.encoder.layer.4.layer_scale1.lambda1', 'backbone.encoder.layer.4.layer_scale2.lambda1', 'backbone.encoder.layer.4.mlp.fc1.bias', 'backbone.encoder.layer.4.mlp.fc1.weight', 'backbone.encoder.layer.4.mlp.fc2.bias', 'backbone.encoder.layer.4.mlp.fc2.weight', 'backbone.encoder.layer.4.norm1.bias', 'backbone.encoder.layer.4.norm1.weight', 'backbone.encoder.layer.4.norm2.bias', 'backbone.encoder.layer.4.norm2.weight', 'backbone.encoder.layer.5.attention.attention.key.bias', 'backbone.encoder.layer.5.attention.attention.key.weight', 'backbone.encoder.layer.5.attention.attention.query.bias', 'backbone.encoder.layer.5.attention.attention.query.weight', 'backbone.encoder.layer.5.attention.attention.value.bias', 'backbone.encoder.layer.5.attention.attention.value.weight', 'backbone.encoder.layer.5.attention.output.dense.bias', 'backbone.encoder.layer.5.attention.output.dense.weight', 'backbone.encoder.layer.5.layer_scale1.lambda1', 'backbone.encoder.layer.5.layer_scale2.lambda1', 'backbone.encoder.layer.5.mlp.fc1.bias', 'backbone.encoder.layer.5.mlp.fc1.weight', 'backbone.encoder.layer.5.mlp.fc2.bias', 'backbone.encoder.layer.5.mlp.fc2.weight', 'backbone.encoder.layer.5.norm1.bias', 'backbone.encoder.layer.5.norm1.weight', 'backbone.encoder.layer.5.norm2.bias', 'backbone.encoder.layer.5.norm2.weight', 'backbone.encoder.layer.6.attention.attention.key.bias', 'backbone.encoder.layer.6.attention.attention.key.weight', 'backbone.encoder.layer.6.attention.attention.query.bias', 'backbone.encoder.layer.6.attention.attention.query.weight', 'backbone.encoder.layer.6.attention.attention.value.bias', 'backbone.encoder.layer.6.attention.attention.value.weight', 'backbone.encoder.layer.6.attention.output.dense.bias', 'backbone.encoder.layer.6.attention.output.dense.weight', 'backbone.encoder.layer.6.layer_scale1.lambda1', 'backbone.encoder.layer.6.layer_scale2.lambda1', 'backbone.encoder.layer.6.mlp.fc1.bias', 'backbone.encoder.layer.6.mlp.fc1.weight', 'backbone.encoder.layer.6.mlp.fc2.bias', 'backbone.encoder.layer.6.mlp.fc2.weight', 'backbone.encoder.layer.6.norm1.bias', 'backbone.encoder.layer.6.norm1.weight', 'backbone.encoder.layer.6.norm2.bias', 'backbone.encoder.layer.6.norm2.weight', 'backbone.encoder.layer.7.attention.attention.key.bias', 'backbone.encoder.layer.7.attention.attention.key.weight', 'backbone.encoder.layer.7.attention.attention.query.bias', 'backbone.encoder.layer.7.attention.attention.query.weight', 'backbone.encoder.layer.7.attention.attention.value.bias', 'backbone.encoder.layer.7.attention.attention.value.weight', 'backbone.encoder.layer.7.attention.output.dense.bias', 'backbone.encoder.layer.7.attention.output.dense.weight', 'backbone.encoder.layer.7.layer_scale1.lambda1', 'backbone.encoder.layer.7.layer_scale2.lambda1', 'backbone.encoder.layer.7.mlp.fc1.bias', 'backbone.encoder.layer.7.mlp.fc1.weight', 'backbone.encoder.layer.7.mlp.fc2.bias', 'backbone.encoder.layer.7.mlp.fc2.weight', 'backbone.encoder.layer.7.norm1.bias', 'backbone.encoder.layer.7.norm1.weight', 'backbone.encoder.layer.7.norm2.bias', 'backbone.encoder.layer.7.norm2.weight', 'backbone.encoder.layer.8.attention.attention.key.bias', 'backbone.encoder.layer.8.attention.attention.key.weight', 'backbone.encoder.layer.8.attention.attention.query.bias', 'backbone.encoder.layer.8.attention.attention.query.weight', 'backbone.encoder.layer.8.attention.attention.value.bias', 'backbone.encoder.layer.8.attention.attention.value.weight', 'backbone.encoder.layer.8.attention.output.dense.bias', 'backbone.encoder.layer.8.attention.output.dense.weight', 'backbone.encoder.layer.8.layer_scale1.lambda1', 'backbone.encoder.layer.8.layer_scale2.lambda1', 'backbone.encoder.layer.8.mlp.fc1.bias', 'backbone.encoder.layer.8.mlp.fc1.weight', 'backbone.encoder.layer.8.mlp.fc2.bias', 'backbone.encoder.layer.8.mlp.fc2.weight', 'backbone.encoder.layer.8.norm1.bias', 'backbone.encoder.layer.8.norm1.weight', 'backbone.encoder.layer.8.norm2.bias', 'backbone.encoder.layer.8.norm2.weight', 'backbone.encoder.layer.9.attention.attention.key.bias', 'backbone.encoder.layer.9.attention.attention.key.weight', 'backbone.encoder.layer.9.attention.attention.query.bias', 'backbone.encoder.layer.9.attention.attention.query.weight', 'backbone.encoder.layer.9.attention.attention.value.bias', 'backbone.encoder.layer.9.attention.attention.value.weight', 'backbone.encoder.layer.9.attention.output.dense.bias', 'backbone.encoder.layer.9.attention.output.dense.weight', 'backbone.encoder.layer.9.layer_scale1.lambda1', 'backbone.encoder.layer.9.layer_scale2.lambda1', 'backbone.encoder.layer.9.mlp.fc1.bias', 'backbone.encoder.layer.9.mlp.fc1.weight', 'backbone.encoder.layer.9.mlp.fc2.bias', 'backbone.encoder.layer.9.mlp.fc2.weight', 'backbone.encoder.layer.9.norm1.bias', 'backbone.encoder.layer.9.norm1.weight', 'backbone.encoder.layer.9.norm2.bias', 'backbone.encoder.layer.9.norm2.weight', 'backbone.layernorm.bias', 'backbone.layernorm.weight', 'head.head.0.bias', 'head.head.0.weight', 'head.head.2.bias', 'head.head.2.weight', 'head.head.4.bias', 'head.head.4.weight', 'head.projection.bias', 'head.projection.weight', 'neck.convs.0.weight', 'neck.convs.1.weight', 'neck.convs.2.weight', 'neck.convs.3.weight', 'neck.fusion_stage.layers.0.projection.bias', 'neck.fusion_stage.layers.0.projection.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer2.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer2.convolution2.weight', 'neck.fusion_stage.layers.1.projection.bias', 'neck.fusion_stage.layers.1.projection.weight', 'neck.fusion_stage.layers.1.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.1.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.1.residual_layer2.convolution1.weight', 'neck.fusion_stage.layers.1.residual_layer2.convolution2.weight', 'neck.fusion_stage.layers.2.projection.bias', 'neck.fusion_stage.layers.2.projection.weight', 'neck.fusion_stage.layers.2.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.2.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.2.residual_layer2.convolution1.weight', 'neck.fusion_stage.layers.2.residual_layer2.convolution2.weight', 'neck.fusion_stage.layers.3.projection.bias', 'neck.fusion_stage.layers.3.projection.weight', 'neck.fusion_stage.layers.3.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.3.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.3.residual_layer2.convolution1.weight', 'neck.fusion_stage.layers.3.residual_layer2.convolution2.weight', 'neck.reassemble_stage.layers.0.projection.bias', 'neck.reassemble_stage.layers.0.projection.weight', 'neck.reassemble_stage.layers.0.resize.bias', 'neck.reassemble_stage.layers.0.resize.weight', 'neck.reassemble_stage.layers.1.projection.bias', 'neck.reassemble_stage.layers.1.projection.weight', 'neck.reassemble_stage.layers.1.resize.bias', 'neck.reassemble_stage.layers.1.resize.weight', 'neck.reassemble_stage.layers.2.projection.bias', 'neck.reassemble_stage.layers.2.projection.weight', 'neck.reassemble_stage.layers.3.projection.bias', 'neck.reassemble_stage.layers.3.projection.weight', 'neck.reassemble_stage.layers.3.resize.bias', 'neck.reassemble_stage.layers.3.resize.weight', 'neck.reassemble_stage.readout_projects.0.0.bias', 'neck.reassemble_stage.readout_projects.0.0.weight', 'neck.reassemble_stage.readout_projects.1.0.bias', 'neck.reassemble_stage.readout_projects.1.0.weight', 'neck.reassemble_stage.readout_projects.2.0.bias', 'neck.reassemble_stage.readout_projects.2.0.weight', 'neck.reassemble_stage.readout_projects.3.0.bias', 'neck.reassemble_stage.readout_projects.3.0.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DPTForDepthEstimation(\n",
       "  (backbone): Dinov2Backbone(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2SdpaAttention(\n",
       "            (attention): Dinov2SdpaSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (neck): DPTNeck(\n",
       "    (reassemble_stage): DPTReassembleStage(\n",
       "      (layers): ModuleList(\n",
       "        (0): DPTReassembleLayer(\n",
       "          (projection): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resize): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(4, 4))\n",
       "        )\n",
       "        (1): DPTReassembleLayer(\n",
       "          (projection): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resize): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (2): DPTReassembleLayer(\n",
       "          (projection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resize): Identity()\n",
       "        )\n",
       "        (3): DPTReassembleLayer(\n",
       "          (projection): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resize): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (readout_projects): ModuleList(\n",
       "        (0-3): 4 x Sequential(\n",
       "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "          (1): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (convs): ModuleList(\n",
       "      (0): Conv2d(48, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (2): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (fusion_stage): DPTFeatureFusionStage(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x DPTFeatureFusionLayer(\n",
       "          (projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (residual_layer1): DPTPreActResidualLayer(\n",
       "            (activation1): ReLU()\n",
       "            (convolution1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (activation2): ReLU()\n",
       "            (convolution2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (residual_layer2): DPTPreActResidualLayer(\n",
       "            (activation1): ReLU()\n",
       "            (convolution1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (activation2): ReLU()\n",
       "            (convolution2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): DPTDepthEstimationHead(\n",
       "    (projection): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (head): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/home/coder/low-power-cv-challenge-track3/outputs/checkpoint-7000/\"\n",
    "model = DPTForDepthEstimation.from_pretrained(model_path)\n",
    "state_dict = load_file(f\"{model_path}/model.safetensors\")\n",
    "new_state_dict = {k.replace(\"module.model.\", \"\"): v for k, v in state_dict.items()}  # Remove the prefix\n",
    "model.load_state_dict(new_state_dict) \n",
    "model.to(\"cuda\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148790404\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import quantize_dynamic\n",
    "model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58965124\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3962965514899738"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "58965124/148790404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from typing import Optional, Tuple, Dict\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class IndoorOutdoorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading indoor and outdoor images along with their labels.\n",
    "    Expected folder structure:\n",
    "    \n",
    "        root_dir/\n",
    "            indoor/              # .jpg images for indoor scenes\n",
    "            indoor_labels/       # .pt label tensors (shape: 384x384) matching indoor images\n",
    "            outdoor/             # .jpg images for outdoor scenes\n",
    "            outdoor_labels/      # .pt label tensors (shape: 384x384) matching outdoor images\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 root_dir: str, \n",
    "                 transform: Optional[transforms.Compose] = None, \n",
    "                 image_size: Tuple[int, int] = (384, 384)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset directory.\n",
    "            transform (torchvision.transforms.Compose, optional): Transformations to apply to images.\n",
    "            image_size (tuple): Desired image size (height, width).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        # Use a default transform if none is provided.\n",
    "        self.transform = transform if transform is not None else transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # List to store tuples of (image_path, label_path)\n",
    "        self.data = []\n",
    "        \n",
    "        # Process indoor images and their labels.\n",
    "        indoor_dir = os.path.join(root_dir, \"indoor\")\n",
    "        indoor_labels_dir = os.path.join(root_dir, \"indoor_labels\")\n",
    "        if os.path.exists(indoor_dir) and os.path.exists(indoor_labels_dir):\n",
    "            for img_name in os.listdir(indoor_dir):\n",
    "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    img_path = os.path.join(indoor_dir, img_name)\n",
    "                    # Construct the label file name by replacing the image extension with .pt\n",
    "                    label_name = os.path.splitext(img_name)[0] + \".pt\"\n",
    "                    label_path = os.path.join(indoor_labels_dir, label_name)\n",
    "                    if os.path.exists(label_path):\n",
    "                        self.data.append((img_path, label_path))\n",
    "        \n",
    "        # Process outdoor images and their labels.\n",
    "        outdoor_dir = os.path.join(root_dir, \"outdoor\")\n",
    "        outdoor_labels_dir = os.path.join(root_dir, \"outdoor_labels\")\n",
    "        if os.path.exists(outdoor_dir) and os.path.exists(outdoor_labels_dir):\n",
    "            for img_name in os.listdir(outdoor_dir):\n",
    "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    img_path = os.path.join(outdoor_dir, img_name)\n",
    "                    label_name = os.path.splitext(img_name)[0] + \".pt\"\n",
    "                    label_path = os.path.join(outdoor_labels_dir, label_name)\n",
    "                    if os.path.exists(label_path):\n",
    "                        self.data.append((img_path, label_path))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        img_path, label_path = self.data[idx]\n",
    "        \n",
    "        # Load the image and convert it to RGB.\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Load the label tensor.\n",
    "        label = torch.load(label_path,map_location=torch.device('cpu'))\n",
    "        \n",
    "        # Return dictionary uses key \"label\" to match your DataLoader usage.\n",
    "        return {\"pixel_values\": image, \"label\": label}\n",
    "\n",
    "def create_data_loader(\n",
    "    data_dir: str,\n",
    "    batch_size: int = 16,\n",
    "    image_size: Tuple[int, int] = (384, 384),\n",
    "    num_workers: int = 4,\n",
    "    transform: Optional[transforms.Compose] = None,\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the combined indoor-outdoor dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Path to the data directory containing 'indoor', 'indoor_labels', 'outdoor', and 'outdoor_labels' subfolders.\n",
    "        batch_size (int): Batch size for the DataLoader.\n",
    "        image_size (tuple): Desired image size for resizing.\n",
    "        num_workers (int): Number of worker processes for data loading.\n",
    "        transform (torchvision.transforms.Compose, optional): Custom transformations for images.\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader: DataLoader for the dataset.\n",
    "    \"\"\"\n",
    "    dataset = IndoorOutdoorDataset(\n",
    "        root_dir=data_dir,\n",
    "        transform=transform,\n",
    "        image_size=image_size\n",
    "    )\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = create_data_loader(data_dir=\"/home/coder/low-power-cv-challenge-track3/data/eval\",batch_size=1,image_size=(384, 384),num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 384, 384])\n",
      "torch.Size([1, 448, 448])\n"
     ]
    }
   ],
   "source": [
    "for data in data_loader:\n",
    "    input_data = data['pixel_values']\n",
    "    print(input_data.shape)\n",
    "    target = data['label']\n",
    "    pred = model(input_data.to(\"cuda\"))\n",
    "    print(pred.predicted_depth.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 448, 448])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.predicted_depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lpcvc-track3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
